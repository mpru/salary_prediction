[
  {
    "objectID": "code/04_transformers.html",
    "href": "code/04_transformers.html",
    "title": "Salary prediction using Large Language Models (Transformers)",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to fit a Text-Based Regression model to predict salary based on the job description provided by each individual. For this, the pre-trained DistilBERT model, a Large Language Model (LLM) based on the Transformer architecture, is retrieved from the Hugging Face platform. The model is fine-tuned on the training data and its performance is evaluated on the test data.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\nWhy Use a LLM\nUsing a LLM like DistilBERT could be a good idea for this problem because it has the ability to capture complex patterns and relationships in text data, which may contain valuable insights for predicting salary beyond traditional numerical features.\nPerformance in Predicting Salary\n\nThe model shows a very good performance when predicting salaries with new data, with \\(R^2: 0.8836\\), which indicates that 88.36% of the variance in the response (salary) is explained by the predictors in the model.\nOn average, the salary predictions made by the model with new data differ from the actual values by USD 16,404.05.\nSimilar values of these metrics in train and test data indicate low risk of overfitting.\n\n\n\n\nSet\n\\(R^2\\)\nRSME\n\n\n\n\nTrain\n0.9094\n14445.76\n\n\nTest\n0.8836\n16404.05\n\n\n\n\nTakeaway points\n\nPretrained models like DistilBERT simplify text-based predictions by reducing manual feature engineering. This implementation is just an example of what can be achieved with such methodologies.\nHyperparameter tuning (learning rate, batch size, epochs, etc.) and alternative preprocessing strategies could enhance performance but were not explored, as this was a proof of concept.\nWith no optimization or validation, DistilBERT performs similarly to linear regression in this case, likely due to the structured nature of salary prediction and the limited dataset size."
  },
  {
    "objectID": "code/04_transformers.html#summary",
    "href": "code/04_transformers.html#summary",
    "title": "Salary prediction using Large Language Models (Transformers)",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to fit a Text-Based Regression model to predict salary based on the job description provided by each individual. For this, the pre-trained DistilBERT model, a Large Language Model (LLM) based on the Transformer architecture, is retrieved from the Hugging Face platform. The model is fine-tuned on the training data and its performance is evaluated on the test data.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\nWhy Use a LLM\nUsing a LLM like DistilBERT could be a good idea for this problem because it has the ability to capture complex patterns and relationships in text data, which may contain valuable insights for predicting salary beyond traditional numerical features.\nPerformance in Predicting Salary\n\nThe model shows a very good performance when predicting salaries with new data, with \\(R^2: 0.8836\\), which indicates that 88.36% of the variance in the response (salary) is explained by the predictors in the model.\nOn average, the salary predictions made by the model with new data differ from the actual values by USD 16,404.05.\nSimilar values of these metrics in train and test data indicate low risk of overfitting.\n\n\n\n\nSet\n\\(R^2\\)\nRSME\n\n\n\n\nTrain\n0.9094\n14445.76\n\n\nTest\n0.8836\n16404.05\n\n\n\n\nTakeaway points\n\nPretrained models like DistilBERT simplify text-based predictions by reducing manual feature engineering. This implementation is just an example of what can be achieved with such methodologies.\nHyperparameter tuning (learning rate, batch size, epochs, etc.) and alternative preprocessing strategies could enhance performance but were not explored, as this was a proof of concept.\nWith no optimization or validation, DistilBERT performs similarly to linear regression in this case, likely due to the structured nature of salary prediction and the limited dataset size."
  },
  {
    "objectID": "code/04_transformers.html#methodology",
    "href": "code/04_transformers.html#methodology",
    "title": "Salary prediction using Large Language Models (Transformers)",
    "section": "Methodology",
    "text": "Methodology\n\nApproaches to Text-Based Regression\nThe main purpose of this problem is to predict salary, a continuous variable. As predictors, we have a combination of traditional features (such as age, experience, education, etc.) and a textual feature, which is the job description provided by each individual. While traditional regression models or other supevised machine learning techniques can use numerical and categorical variables effectively, they can’t use text data unless it is processed before to create new features. However, the available textual job description likely contains important information as it is, and could improve predictive performance, making it an essential variable to consider.\nHence, to predict a continuous variable from text data, two main approaches can be considered:\n\nTraditional Methods\nThis approach involves processing the text by tokenizing it, cleaning it (e.g., removing stopwords or punctuation), and extracting features based on term statistics such as word counts, term frequencies, or TF-IDF (term frequency-inverse document frequency). These features can then be fed into traditional regression models or machine learning algorithms.\nModern Approach Using Large Language Models (LLMs)\nLLMs represent a more recent and powerful method for working with textual data. These models, trained on massive text corpora, can generate dense vector representations (embeddings) of text that capture its semantic meaning. LLMs, such as BERT (Bidirectional Encoder Representations from Transformers), are particularly effective because they utilize deep learning architectures to process and understand context in language.\n\nThe later is the approach followed for this problem.\n\n\nTransformer Architecture and DistilBERT\nThe transformer architecture, introduced by Vaswani et al. (2017), revolutionized the field of natural language processing (NLP) with its encoder-decoder design. The encoder of a Transformer is composed of multiple layers of self-attention mechanisms and feed-forward neural networks, designed to process input sequences and generate a contextualized representation for each token by considering its relationship with all other tokens in the sequence. The decoder, on the other hand, also uses self-attention but incorporates an additional mechanism called encoder-decoder attention, which allows it to focus on relevant parts of the encoder’s output while generating an output sequence token by token, typically used in tasks like text generation or translation. Together, these components enable Transformers to effectively model both input and output sequences with contextual understanding.\nBERT, one of the most prominent models built on transformers, uses only the encoder portion of the architecture. It processes text bidirectionally, meaning it considers both the left and right contexts of a word in a sentence simultaneously. This makes it well-suited for various NLP tasks, including classification, question answering, and regression.\nDistilBERT, a lighter and faster version of BERT, is a distilled model that retains approximately 97% of BERT’s performance while being smaller and more computationally efficient. DistilBERT is particularly useful in tasks requiring large-scale deployment or limited computational resources. In this problem, I used DistilBERT for the text regression task, leveraging its pretrained embeddings and fine-tuning capabilities.\n\n\nSteps for Model Implementation\nTo implement a text regression model with DistilBERT, I followed these steps:\n\nModel and Tokenizer Selection\nThe pretrained DistilBERT model and its corresponding tokenizer is downloaded from the Hugging Face Model Hub. The tokenizer converts raw text into input tokens, which are numerical representations that the model can process.\nData Preprocessing\nThe job descriptions were tokenized using the DistilBERT tokenizer, ensuring proper formatting (e.g., truncation and padding) to meet the model’s input size requirements.\nFine-Tuning\nDistilBERT is fine-tuned on our dataset to adapt the pretrained model to the specific task of predicting salary. The output layer was modified to produce a single continuous value corresponding to salary. This required using a regression loss function, such as Mean Squared Error (MSE).\nTraining\nThe model was trained with the job descriptions as input and the corresponding salaries as the target variable. Traditional features (age, experience, education, etc.) were integrated into the model as text concatenated to the biginning of the job description string.\nEvaluation\nAfter training, the model’s performance was evaluated on a separate test set using standard regression metrics such as R-squared and Root Mean Squared Error (RMSE).\n\n\n\nKey Considerations\nThe integration of textual data into predictive models introduces complexity but also offers significant potential for improved accuracy. The use of pretrained LLMs like DistilBERT simplifies this process by eliminating the need for extensive manual feature engineering and Hugging Face’s Model Hub provides an accessible platform for downloading, customizing, and deploying state-of-the-art language models.\nHowever, for this problem, as can be seen later, the performance of the DistilBERT model is equivalent to the classic linear regression model, not showing an improvement over it. This could be due to several factors. First, DistilBERT, like other transformer-based models, excels at capturing complex patterns in large, unstructured text data, but may not provide significant benefits for tasks with limited or highly structured input, such as the salary prediction in this case. Moreover, transformer models require substantial computational resources, and might not be as effective when the training dataset is not large enough to leverage their power.\nAdditionally, the performance might be improved with the validation and tuning of the model’s hyperparameters. Key aspects of the DistilBERT model, such as learning rate, batch size, sequence length, and the number of training epochs, could be further optimized using techniques like cross-validation or grid search. Moreover, different strategies for the pre-processing of data or even more epochs for the training could be evaluated. It is important to note that since this task was primarily a proof of concept and an example, the process of improving the model’s performance was not pursued further. In real-world applications, the next steps would involve carefully tuning these parameters and possibly exploring the use of additional features or more data.\n\n\nReferences\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 30 (NeurIPS 2017). Retrieved from https://arxiv.org/abs/1706.03762\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. Retrieved from https://arxiv.org/abs/1910.01108"
  },
  {
    "objectID": "code/04_transformers.html#libraries-and-data",
    "href": "code/04_transformers.html#libraries-and-data",
    "title": "Salary prediction using Large Language Models (Transformers)",
    "section": "Libraries and data",
    "text": "Libraries and data\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.getcwd(), \"code\"))\nfrom modulos import metrics, scatter_plot_real_vs_pred\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset #, load_dataset, load_from_disk\nfrom sklearn.metrics import root_mean_squared_error, r2_score\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nimport joblib\n\n\n# Load data\ntrain_df = pd.read_csv('../data/train_set.csv')\ntest_df = pd.read_csv('../data/test_set.csv')\n\nFor using these libraries, the predictor column must be called text in the dataframe, while the response must be called label and have float format.\nAlso, it is known that text-based regression models like this may have a poor performance when the response exhibits large values. To overcome this, I take a log10 transformation of the salaries. Other approaches or types of normalizations could be also considered and validated.\nFinally, dataframe have to be transformed to the format required for the transformers library\n\ntrain_df['label'] = np.log10(train_df['salary'].clip(lower=1))\ntest_df['label'] = np.log10(test_df['salary'].clip(lower=1))\ntrain_df = train_df[['salary', 'label', 'text']]\ntest_df = test_df[['salary', 'label', 'text']]\n\ntrain_dataset = Dataset.from_pandas(train_df, preserve_index = False)\ntest_dataset = Dataset.from_pandas(test_df, preserve_index = False)\n\ntrain_dataset[0]   # Primer ejemplo del conjunto de entrenamiento\ntest_dataset[0]    # Primer ejemplo del conjunto de prueba\n\n{'salary': 150000.0,\n 'label': 5.176091259055681,\n 'text': 'Age: 44.0 - Gender: Female - Education level: PhD - Title: Senior Product Designer - Years of experience: 15.0 - Job description: As a 44-year-old Senior Product Designer with a PhD and 15 years of experience, I specialize in creating innovative, user-centric designs that drive product success. My expertise lies in blending aesthetics with functionality to ensure exceptional user experiences. I have worked on a diverse range of projects, from conceptualizing new products to refining existing ones, and thrive in collaborative environments. My deep understanding of design principles, user research, and industry trends allows me to solve complex design challenges effectively. I am passionate about continuous learning and mentoring the next generation of designers.'}"
  },
  {
    "objectID": "code/04_transformers.html#tokenizer",
    "href": "code/04_transformers.html#tokenizer",
    "title": "Salary prediction using Large Language Models (Transformers)",
    "section": "Tokenizer",
    "text": "Tokenizer\nWe load the tokenizer and see examples of how it works. The tokenizer could also be fine-tuned, but we don’t do it.\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Ejemplos\ntokenizer(\"I am a 39-year-old Senior Project Coordinator with a Master's degree\")\n[tokenizer.decode(i) for i in tokenizer(\"I am a 39-year-old Senior Project Coordinator with a Master's degree\")['input_ids']]\n\n# Función de tokenización\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding = \"max_length\", truncation = True)\n\n# Aplicar tokenización a los datasets\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched = True)\ntokenized_test_dataset = test_dataset.map(tokenize_function, batched = True)"
  },
  {
    "objectID": "code/04_transformers.html#training",
    "href": "code/04_transformers.html#training",
    "title": "Salary prediction using Large Language Models (Transformers)",
    "section": "Training",
    "text": "Training\nNow we call the pretrained mode, “distilbert-base-uncased”, available on Hugging Face. The argument num_labels = 1 is used to stablish that the final layer has only 1 output neuron, which is neccesary for performing this regression task.\n\n# Model selection\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = 1)\nmodel.resize_token_embeddings(len(tokenizer))\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nEmbedding(30522, 768, padding_idx=0)\n\n\nThe following function defines the calculation of performance metrics (\\(R^2\\) and RMSE), which will be used to track training progress during execution.\n\n# Definir métricas a usar\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Asegurarse de que las predicciones y etiquetas son de tipo float\n    predictions = predictions.flatten()  # Flatten para asegurar que la forma sea correcta\n    labels = labels.flatten()\n    rmse = root_mean_squared_error(labels, predictions)\n    r2 = r2_score(labels, predictions)\n    return {\"rmse\": rmse, \"r2\": r2}\n\nNext we perform the training itself. For this, all the required hyperparameters are specified. Note that it is convenient to validate them, but we don’t do it in this project.\n\n# Configuración del entrenamiento\ntraining_args = TrainingArguments(\n    output_dir = \"test_trainer\",\n    logging_strategy = \"epoch\",\n    eval_strategy = \"epoch\",\n    per_device_train_batch_size = 8,\n    per_device_eval_batch_size = 8,\n    num_train_epochs = 10,\n    save_total_limit = 2,\n    save_strategy = \"epoch\",\n    load_best_model_at_end = True\n)\n\n# Entrenador\ntrainer = Trainer(\n    model = model,\n    args = training_args,\n    train_dataset = tokenized_train_dataset,\n    eval_dataset = tokenized_test_dataset,\n    compute_metrics = compute_metrics\n)\n\ntorch.cuda.empty_cache()\ntrainer.train()\n\nWith the trained model, we gather the predictions made for the complete training set, and convert back to the original scale, in USD (by taking anti-logarithm). We save the results in a data.frame for further exploration.\n\n# guardo los rtdos para que no se entrene el modelo con cada render del documento,\n# esto podria solucionarlo usando cache\ntrain_pred = trainer.predict(tokenized_train_dataset).predictions.flatten()\ntrain_pred_orig = 10 ** train_pred\nrtdos_train = pd.DataFrame({\n    'logsalary': train_df['label'], \n    'logsalary_pred': train_pred,\n    'salary': train_df['salary'], \n    'salary_pred': train_pred_orig,\n})\njoblib.dump(rtdos_train, '../model_outputs/train_predictions_distilbert.pkl')\n\nSince the \\(R^2\\) and RMSE metrics reported by the training process refer to the transformed response, we recalculate them for the training set in the original scale (USD), comparing the true values of salary and the predicted ones.\n\nrtdos_train = joblib.load('../model_outputs/train_predictions_distilbert.pkl')\nmetrics(rtdos_train['salary'], rtdos_train['salary_pred'], \"train\")\n\nMétricas para train:\n - R2: 0.9094\n - RMSE: 14445.7633\n\n\n\n\nscatter_plot_real_vs_pred(rtdos_train, 'salary', 'salary_pred', \"Training data\", label = \" - Salary (USD)\")\n\n\n\n\n\n\n\n\nThe model shows a very good performance, with \\(R^2: 0.9094\\), which indicates that 90.94% of the variance in the response (salary) is explained by the predictors in the model.\nOn average, the salary predictions made by the model differ from the actual values by USD 14445.76.\nThe model underestimates the response for outlier cases with very high real salaries."
  },
  {
    "objectID": "code/04_transformers.html#prediction-in-test-data",
    "href": "code/04_transformers.html#prediction-in-test-data",
    "title": "Salary prediction using Large Language Models (Transformers)",
    "section": "Prediction in test data",
    "text": "Prediction in test data\nThe model shows a very good performance when predicting salaries with new data, with \\(R^2: 0.8836\\), which indicates that 88.36% of the variance in the response (salary) is explained by the predictors in the model.\nOn average, the salary predictions made by the model with new data differ from the actual values by USD 16,404.05.\nAs expected, these metrics are slightly worse than the ones for the train data, with no indication of overfitting.\n\ntest_pred = trainer.predict(tokenized_test_dataset).predictions.flatten()\ntest_pred_orig = 10 ** test_pred\nrtdos_test = pd.DataFrame({\n    'logsalary': test_df['label'], \n    'logsalary_pred': test_pred,\n    'salary': test_df['salary'], \n    'salary_pred': test_pred_orig,\n})\njoblib.dump(rtdos_test, '../model_outputs/test_predictions_distilbert.pkl')\n\n\nrtdos_test = joblib.load('../model_outputs/test_predictions_distilbert.pkl')\n\n# Métricas para el conjunto de prueba\nmetrics(rtdos_test['salary'], rtdos_test['salary_pred'], \"test\")\n\nMétricas para test:\n - R2: 0.8836\n - RMSE: 16404.0454\n\n\n\n\nscatter_plot_real_vs_pred(rtdos_test, 'salary', 'salary_pred', \"Testing data\", label = \" - Salary (USD)\")"
  },
  {
    "objectID": "code/04_transformers.html#saving-the-trained-model",
    "href": "code/04_transformers.html#saving-the-trained-model",
    "title": "Salary prediction using Large Language Models (Transformers)",
    "section": "Saving the trained model",
    "text": "Saving the trained model\n\n# Guardar el modelo\nmodel.save_pretrained('../model_outputs/mod_distilbert')\n\n# Guardar el tokenizador\ntokenizer.save_pretrained('../model_outputs/tokenizador')"
  },
  {
    "objectID": "code/00_cleansing.html",
    "href": "code/00_cleansing.html",
    "title": "Data cleansing and transformation",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to perform an initial inspection of the data, addressing missing values, inconsistencies in observations, reducing the number of levels in categorical variables, etc. The output will be a cleaned dataset to be used in subsequent stages of the analysis.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nValidation\n\nCases with missings in the response variable salary are omitted.\nIt is identified that the minimum observed value for salary needs to be corrected.\n\nThrough a visual inspection of “title” (174 distinct values), it is deemed appropriate to transform it into a new variable, “title_cat”, which takes on 4 categories: “Junior,” “Senior,” “Leadership,” or “Other,” based on the words appearing in “title” and the rules described below.\n\nMissing values\n\nMissing values for age are imputed using regular expressions, searching the job description for expressions like “years old”.\n\nMissing values for education are imputed using regular expressions, searching the job description for words like “Master’s,” “Bachelor’s,” or “PhD”.\n\nMissing values for title_cat are manually imputed (this can be improved) based on the job description.\n\nMissing values for gender cannot be inferred from the job description.\n\nMissing values for job description are not addressed either.\n\nJob description (text data)\n\nThis column will be used to train a text regression model (a transformer-based model where the response variable is numerical, and the only explanatory variable is a text string).\n\nAs it is, it can be used, but to make better use of the available information, I prepend a string created from the values of the other variables to the beginning of the job description string."
  },
  {
    "objectID": "code/00_cleansing.html#summary",
    "href": "code/00_cleansing.html#summary",
    "title": "Data cleansing and transformation",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to perform an initial inspection of the data, addressing missing values, inconsistencies in observations, reducing the number of levels in categorical variables, etc. The output will be a cleaned dataset to be used in subsequent stages of the analysis.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nValidation\n\nCases with missings in the response variable salary are omitted.\nIt is identified that the minimum observed value for salary needs to be corrected.\n\nThrough a visual inspection of “title” (174 distinct values), it is deemed appropriate to transform it into a new variable, “title_cat”, which takes on 4 categories: “Junior,” “Senior,” “Leadership,” or “Other,” based on the words appearing in “title” and the rules described below.\n\nMissing values\n\nMissing values for age are imputed using regular expressions, searching the job description for expressions like “years old”.\n\nMissing values for education are imputed using regular expressions, searching the job description for words like “Master’s,” “Bachelor’s,” or “PhD”.\n\nMissing values for title_cat are manually imputed (this can be improved) based on the job description.\n\nMissing values for gender cannot be inferred from the job description.\n\nMissing values for job description are not addressed either.\n\nJob description (text data)\n\nThis column will be used to train a text regression model (a transformer-based model where the response variable is numerical, and the only explanatory variable is a text string).\n\nAs it is, it can be used, but to make better use of the available information, I prepend a string created from the values of the other variables to the beginning of the job description string."
  },
  {
    "objectID": "code/00_cleansing.html#libraries-and-modules",
    "href": "code/00_cleansing.html#libraries-and-modules",
    "title": "Data cleansing and transformation",
    "section": "Libraries and modules",
    "text": "Libraries and modules\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.getcwd(), \"code\"))\nfrom modulos import explore_missing\nimport pandas as pd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "code/00_cleansing.html#read-and-merge-data-sources",
    "href": "code/00_cleansing.html#read-and-merge-data-sources",
    "title": "Data cleansing and transformation",
    "section": "Read and merge data sources",
    "text": "Read and merge data sources\n\n# Read data\npeople = pd.read_csv(\"../data/people.csv\")\ndescr = pd.read_csv(\"../data/descriptions.csv\")\nsalary = pd.read_csv(\"../data/salary.csv\")\n\n# merge\ndatos = (\n    people\n    .merge(descr, on=\"id\", how=\"outer\")\n    .merge(salary, on=\"id\", how=\"outer\")\n    .rename(columns={\n        \"Age\": \"age\",\n        \"Gender\": \"gender\",\n        \"Education Level\": \"educ\",\n        \"Job Title\": \"title\",\n        \"Years of Experience\": \"exp\",\n        \"Description\": \"descr\",\n        \"Salary\": \"salary\"\n    })\n    .dropna(subset=[\"salary\"])  # Filtrar casos donde salary es NaN\n)\n\ndatos.head()\n\n\n\n\n\n\n\n\nid\nage\ngender\neduc\ntitle\nexp\ndescr\nsalary\n\n\n\n\n0\n0\n32.0\nMale\nBachelor's\nSoftware Engineer\n5.0\nI am a 32-year-old male working as a Software ...\n90000.0\n\n\n1\n1\n28.0\nFemale\nMaster's\nData Analyst\n3.0\nI am a 28-year-old data analyst with a Master'...\n65000.0\n\n\n2\n2\n45.0\nMale\nPhD\nSenior Manager\n15.0\nI am a 45-year-old Senior Manager with a PhD a...\n150000.0\n\n\n3\n3\n36.0\nFemale\nBachelor's\nSales Associate\n7.0\nI am a 36-year-old female Sales Associate with...\n60000.0\n\n\n4\n4\n52.0\nMale\nMaster's\nDirector\n20.0\nI am a 52-year-old male with over two decades ...\n200000.0"
  },
  {
    "objectID": "code/00_cleansing.html#validation",
    "href": "code/00_cleansing.html#validation",
    "title": "Data cleansing and transformation",
    "section": "Validation",
    "text": "Validation\n\nCategorical values\n\n# values in categorical variables\ndatos['gender'].value_counts(dropna=False)\ndatos['educ'].value_counts(dropna=False)\ndatos['title'].value_counts(dropna=False).sort_values(ascending=False)\n\n# new column title_cat\ndatos['title_cat'] = datos['title'].apply(\n    lambda x: (\"Senior\" if \"Senior\" in x else\n               \"Junior\" if \"Junior\" in x else\n               \"Leadership\" if any(word in x for word in [\"Manager\", \"CEO\", \"Chief\", \"Director\", \"Principal\", \"Associate\"]) else\n               \"Other\") if pd.notna(x) else pd.NA\n)\n\n\n\nNumerical variables\n\n# values in numerical variables\ndatos[['salary', 'age', 'exp']].describe().style.format(precision=2)\n\n\n\n\n\n\n \nsalary\nage\nexp\n\n\n\n\ncount\n373.00\n370.00\n373.00\n\n\nmean\n100577.35\n37.44\n10.03\n\n\nstd\n48240.01\n7.08\n6.56\n\n\nmin\n350.00\n23.00\n0.00\n\n\n25%\n55000.00\n31.25\n4.00\n\n\n50%\n95000.00\n36.00\n9.00\n\n\n75%\n140000.00\n44.00\n15.00\n\n\nmax\n250000.00\n53.00\n25.00\n\n\n\n\n\n\n# salary validation\nplt.figure(figsize=(8, 6))\nplt.hist(datos['salary'], bins=30, color='blue', edgecolor='black', alpha=0.7)\nplt.title('Salary distribution')\nplt.xlabel('Salary')\nplt.ylabel('Frequency')\nplt.show()\n\n\n# min de salario es muy pequeño, revisar\ndatos.loc[datos['salary'] == datos['salary'].min()].values\n\n# se debe corregir este valor minimo de salario, por la descripcion puede ser 35000\ndatos['salary'] = datos['salary'].replace(350, 35000) # todo: mejorar\n\n# por las dudas veo el maximo\ndatos.loc[datos['salary'] == datos['salary'].max()]\n\n\n\n\n\n\n\n\nid\nage\ngender\neduc\ntitle\nexp\ndescr\nsalary\ntitle_cat\n\n\n\n\n30\n30\n50.0\nMale\nBachelor's\nCEO\n25.0\nI am a 50-year-old male with a Bachelor's degr...\n250000.0\nLeadership\n\n\n83\n83\n52.0\nMale\nPhD\nChief Technology Officer\n24.0\nI am a 52-year-old male with a PhD and over tw...\n250000.0\nLeadership"
  },
  {
    "objectID": "code/00_cleansing.html#missing-values",
    "href": "code/00_cleansing.html#missing-values",
    "title": "Data cleansing and transformation",
    "section": "Missing values",
    "text": "Missing values\n\nexplore_missing(datos)\n\nComplete cases: 358\nProportion of complete cases: 0.9597855227882037\nProportion of complete cells: 0.9946380697050938\n\nMissing values by column:\n    Variable  Missing Count  Missing Proportion\n0         id              0            0.000000\n1        age              3            0.008043\n2     gender              3            0.008043\n3       educ              3            0.008043\n4      title              3            0.008043\n5        exp              0            0.000000\n6      descr              3            0.008043\n7     salary              0            0.000000\n8  title_cat              3            0.008043\n\n\n\n# Age imputation\nna_age_idx = datos['age'].isna()\nna_age_descr = datos.loc[na_age_idx, 'descr']\nedades = na_age_descr.str.extract(r\"(\\d{2})[- ]year[- ]old\")[0].astype(float)\ndatos.loc[na_age_idx, 'age'] = edades\n\n# Educ imputation\ndatos['educ'] = datos.apply(\n    lambda row: \"Master's\" if pd.isna(row['educ']) and \"Master\" in row['descr'] else\n                \"Bachelor's\" if pd.isna(row['educ']) and \"Bachelor\" in row['descr'] else\n                \"PhD\" if pd.isna(row['educ']) and \"Ph\" in row['descr'] else\n                row['educ'],\n    axis=1\n)\n\n# Job Title imputation (manual, todo: mejorar)\ndatos.loc[datos['title'].isna(), 'title_cat'] = ['Other', 'Leadership', 'Senior']\n\nexplore_missing(datos)\n\nComplete cases: 363\nProportion of complete cases: 0.9731903485254692\nProportion of complete cells: 0.9970211498361632\n\nMissing values by column:\n    Variable  Missing Count  Missing Proportion\n0         id              0            0.000000\n1        age              1            0.002681\n2     gender              3            0.008043\n3       educ              0            0.000000\n4      title              3            0.008043\n5        exp              0            0.000000\n6      descr              3            0.008043\n7     salary              0            0.000000\n8  title_cat              0            0.000000"
  },
  {
    "objectID": "code/00_cleansing.html#job-description-text-data",
    "href": "code/00_cleansing.html#job-description-text-data",
    "title": "Data cleansing and transformation",
    "section": "Job description (text data)",
    "text": "Job description (text data)\n\n# Crear nueva columna con descripción completa\ndatos['text'] = (\n    \"Age: \" + datos['age'].astype(str) +\n    \" - Gender: \" + datos['gender'].astype(str) +\n    \" - Education level: \" + datos['educ'].astype(str) +\n    \" - Title: \" + datos['title'].astype(str) +\n    \" - Years of experience: \" + datos['exp'].astype(str) +\n    \" - Job description: \" + datos['descr'].astype(str)\n)"
  },
  {
    "objectID": "code/00_cleansing.html#saving-transformed-data",
    "href": "code/00_cleansing.html#saving-transformed-data",
    "title": "Data cleansing and transformation",
    "section": "Saving transformed data",
    "text": "Saving transformed data\n\nSave clean data\n\ndatos = datos.drop(columns=[\"id\"])\ndatos.to_csv('../data/clean_data.csv', index = False)\n\n\n\nCreate sample data\nThese files will can be used as examples when using the app for making predictions with the trained models.\n\n# Seleccionar al azar 5 filas y columnas específicas\nsample_rows = datos.sample(n = 5, random_state = 200)\nsample_rows[['age', 'gender', 'educ', 'title_cat', 'exp']].to_csv('../data/sample_data_vars.csv', index = False)\nsample_rows[['text']].to_csv('../data/sample_data_onlytext.csv', index = False)\nsample_rows[['age', 'gender']].to_csv('../data/sample_data_wrong.csv', index = False)"
  },
  {
    "objectID": "code/02_train_test_split.html",
    "href": "code/02_train_test_split.html",
    "title": "Data partitions",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to split the available dataset into a training set (for fitting predictive models) and a test set (for validating them and calculating performance metrics).\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nDue to the small size of the dataset, a two-partition approach (train and test) will be used. Other approaches with more partitions (e.g., train, validation, and test) could be considered, as they may offer advantages in producing unbiased estimates of test error.\n\nIt is important to ensure that the randomness in creating the partitions does not lead to differing distributions of the response variable between train and test. Therefore, the partitions will be created by stratifying based on the salary variable, using percentiles.\n\nAll models to be fitted will use the same partitions.\n\nAn 80/20 split was chosen, reserving 80% for train and 20% for test."
  },
  {
    "objectID": "code/02_train_test_split.html#summary",
    "href": "code/02_train_test_split.html#summary",
    "title": "Data partitions",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to split the available dataset into a training set (for fitting predictive models) and a test set (for validating them and calculating performance metrics).\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nDue to the small size of the dataset, a two-partition approach (train and test) will be used. Other approaches with more partitions (e.g., train, validation, and test) could be considered, as they may offer advantages in producing unbiased estimates of test error.\n\nIt is important to ensure that the randomness in creating the partitions does not lead to differing distributions of the response variable between train and test. Therefore, the partitions will be created by stratifying based on the salary variable, using percentiles.\n\nAll models to be fitted will use the same partitions.\n\nAn 80/20 split was chosen, reserving 80% for train and 20% for test."
  },
  {
    "objectID": "code/02_train_test_split.html#libraries-and-modules",
    "href": "code/02_train_test_split.html#libraries-and-modules",
    "title": "Data partitions",
    "section": "Libraries and modules",
    "text": "Libraries and modules\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "code/02_train_test_split.html#create-partitions",
    "href": "code/02_train_test_split.html#create-partitions",
    "title": "Data partitions",
    "section": "Create partitions",
    "text": "Create partitions\n\n# Load data\ndatos = pd.read_csv('../data/clean_data.csv')\n\n# usar percentiles para respetar la distribucion de salary en ambos sets\nintervalos = pd.qcut(x = datos['salary'], q = 5, labels = False, duplicates = 'drop')\n\n# Split\ntrain, test = train_test_split(datos, test_size = 0.2, random_state = 89, stratify = intervalos)\n\n# Save\ntrain.to_csv('../data/train_set.csv', index = False)\ntest.to_csv('../data/test_set.csv', index = False)"
  },
  {
    "objectID": "code/02_train_test_split.html#checking-similar-distribution-for-salary",
    "href": "code/02_train_test_split.html#checking-similar-distribution-for-salary",
    "title": "Data partitions",
    "section": "Checking similar distribution for salary",
    "text": "Checking similar distribution for salary\n\ntrain['salary'].describe()\n\ncount       298.000000\nmean     100486.577181\nstd       48077.674467\nmin       30000.000000\n25%       55000.000000\n50%       95000.000000\n75%      140000.000000\nmax      250000.000000\nName: salary, dtype: float64\n\n\n\ntest['salary'].describe()\n\ncount        75.000000\nmean     101400.000000\nstd       48403.986881\nmin       35000.000000\n25%       57500.000000\n50%       95000.000000\n75%      140000.000000\nmax      220000.000000\nName: salary, dtype: float64"
  },
  {
    "objectID": "code/05_comparisons.html",
    "href": "code/05_comparisons.html",
    "title": "Model comparisons",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this section is to summarize the metrics that describe the performance of the trained models for salary prediction.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nBoth trained models showed similar performance, with the linear regression model slightly outperforming the DistilBERT model (LLM, Transformer-based).\n\nSince the latter was trained with few epochs and without any validation or hyperparameter optimization, it is expected that its performance could be significantly improved."
  },
  {
    "objectID": "code/05_comparisons.html#summary",
    "href": "code/05_comparisons.html#summary",
    "title": "Model comparisons",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this section is to summarize the metrics that describe the performance of the trained models for salary prediction.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nBoth trained models showed similar performance, with the linear regression model slightly outperforming the DistilBERT model (LLM, Transformer-based).\n\nSince the latter was trained with few epochs and without any validation or hyperparameter optimization, it is expected that its performance could be significantly improved."
  },
  {
    "objectID": "code/05_comparisons.html#performance-metrics",
    "href": "code/05_comparisons.html#performance-metrics",
    "title": "Model comparisons",
    "section": "Performance metrics",
    "text": "Performance metrics\nIn this problem, \\(R^2\\) (coefficient of determination) and RMSE (root mean squared error) were used to evaluate model performance:\n\n\\(R^2\\) measures the proportion of variance in the response variable (salary) explained by the model. A value close to 1 indicates a strong predictive ability, while a value near 0 suggests the model does not explain much of the variability. This metric is useful for assessing how well the model captures overall trends in the data.\nRMSE quantifies the average error in salary predictions, penalizing larger deviations more heavily than smaller ones. It provides an intuitive interpretation of model accuracy in the same units as the response variable (USD), making it easy to compare errors across different models.\n\nThese metrics were chosen because they complement each other: R² evaluates the explanatory power of the model, while RMSE provides a direct measure of prediction accuracy. Together, they offer a comprehensive assessment of model performance."
  },
  {
    "objectID": "code/05_comparisons.html#results",
    "href": "code/05_comparisons.html#results",
    "title": "Model comparisons",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\\(R^2\\)\nLin. Reg.\nLLM\n\n\n\n\nTrain\n0.9231\n0.9094\n\n\nTest\n0.8912\n0.8836\n\n\n\n\n\n\n\n\n\nRMSE\nLin. Reg.\nLLM\n\n\n\n\nTrain\n13372.68\n14445.76\n\n\nTest\n15819.82\n16404.05"
  },
  {
    "objectID": "website/index.html",
    "href": "website/index.html",
    "title": "Salary prediction: a data science sample project",
    "section": "",
    "text": "work in progress"
  },
  {
    "objectID": "website/presentation.html#diapo1",
    "href": "website/presentation.html#diapo1",
    "title": "titulo",
    "section": "diapo1",
    "text": "diapo1\ncontenido"
  },
  {
    "objectID": "website/presentation.html#diapo2",
    "href": "website/presentation.html#diapo2",
    "title": "titulo",
    "section": "diapo2",
    "text": "diapo2\ncontenido\n\n\n\nBack to home"
  },
  {
    "objectID": "website/shiny_embed.html",
    "href": "website/shiny_embed.html",
    "title": "Making salary predictions for new cases",
    "section": "",
    "text": "Caution\n\n\n\nThis page might take a minute to load as the server and environment get ready for making new predictions."
  },
  {
    "objectID": "code/03_linear_regression.html",
    "href": "code/03_linear_regression.html",
    "title": "Salary prediction using a linear regression model",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to fit a multiple linear regression model to predict the response variable salary based on the remaining available variables. Additionally, statistical inference methodologies are applied to estimate and quantify the level of association between the predictors and the response, measuring the degree of confidence in the conclusions. The technical conditions that the model must meet to ensure valid conclusions are assessed, and finally, the model is used to make predictions on the test data to evaluate its performance.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\nWhy Use Regression\n\nAlthough it is a classic and simple methodology, regression models are highly useful and flexible, offering the advantage of clearly interpreting the effect of predictors on the response variable, along with probabilistic conclusions.\n\nThis model serves as a baseline for comparison, any more advanced technique should outperform it to be considered.\n\nPerformance in Predicting Salary\n\nThe model shows a very good performance when predicting salaries with new data, with \\(R^2: 0.8912\\), which indicates that 89.12% of the variance in the response (salary) is explained by the predictors in the model.\nOn average, the salary predictions made by the model with new data differ from the actual values by USD 15,819.82.\nSimilar values of these metrics in train and test data indicate low risk of overfitting.\n\n\n\n\nSet\n\\(R^2\\)\nRSME\n\n\n\n\nTrain\n0.9231\n13372,68\n\n\nTest\n0.8912\n15819.82\n\n\n\n\nResults\n\nThe overall model is highly significant (\\(F\\)-stat: 428.9, p-value &lt; 0.0001).\n\nAll predictors make significant contributions, as supported by ANOVA results.\n\nNo major violations of model assumptions were detected.\nKey effects. On average, and holding all other factors constant:\n\nA Master’s degree increases salary by $18,390, and a PhD increases it by $23,180 compared to individuals with a Bachelor’s degree.\nBeing in a Leadership role increases salary by $14,050, and being a Senior employee increases it by $13,590, compared to being a Junior.\nEach additional year of age increases salary by $2540.94 and each additional year of experience increases it by $2497.27\nMen earn $7999.69 more than women."
  },
  {
    "objectID": "code/03_linear_regression.html#summary",
    "href": "code/03_linear_regression.html#summary",
    "title": "Salary prediction using a linear regression model",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to fit a multiple linear regression model to predict the response variable salary based on the remaining available variables. Additionally, statistical inference methodologies are applied to estimate and quantify the level of association between the predictors and the response, measuring the degree of confidence in the conclusions. The technical conditions that the model must meet to ensure valid conclusions are assessed, and finally, the model is used to make predictions on the test data to evaluate its performance.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\nWhy Use Regression\n\nAlthough it is a classic and simple methodology, regression models are highly useful and flexible, offering the advantage of clearly interpreting the effect of predictors on the response variable, along with probabilistic conclusions.\n\nThis model serves as a baseline for comparison, any more advanced technique should outperform it to be considered.\n\nPerformance in Predicting Salary\n\nThe model shows a very good performance when predicting salaries with new data, with \\(R^2: 0.8912\\), which indicates that 89.12% of the variance in the response (salary) is explained by the predictors in the model.\nOn average, the salary predictions made by the model with new data differ from the actual values by USD 15,819.82.\nSimilar values of these metrics in train and test data indicate low risk of overfitting.\n\n\n\n\nSet\n\\(R^2\\)\nRSME\n\n\n\n\nTrain\n0.9231\n13372,68\n\n\nTest\n0.8912\n15819.82\n\n\n\n\nResults\n\nThe overall model is highly significant (\\(F\\)-stat: 428.9, p-value &lt; 0.0001).\n\nAll predictors make significant contributions, as supported by ANOVA results.\n\nNo major violations of model assumptions were detected.\nKey effects. On average, and holding all other factors constant:\n\nA Master’s degree increases salary by $18,390, and a PhD increases it by $23,180 compared to individuals with a Bachelor’s degree.\nBeing in a Leadership role increases salary by $14,050, and being a Senior employee increases it by $13,590, compared to being a Junior.\nEach additional year of age increases salary by $2540.94 and each additional year of experience increases it by $2497.27\nMen earn $7999.69 more than women."
  },
  {
    "objectID": "code/03_linear_regression.html#methodology",
    "href": "code/03_linear_regression.html#methodology",
    "title": "Salary prediction using a linear regression model",
    "section": "Methodology",
    "text": "Methodology\nMultiple linear regression is a statistical method used to model the relationship between a numerical response variable (Y) and multiple explanatory variables (X_1, X_2, …, X_k). The general form of the model is expressed as:\n\\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_kX_k + \\epsilon\\]\nwhere \\(\\beta_0\\) represents the intercept, \\(\\beta_1, \\beta_2, ..., \\beta_k\\) are the regression coefficients associated with each explanatory variable, and \\(\\epsilon\\) is the error term, which captures the variability in the response variable that is not explained by the predictors.\nThis model is widely used for both predictive and inferential purposes. Inferentially, the coefficients \\(\\beta_i\\) provide information about the strength and direction of the relationship between each explanatory variable and the response variable. Hypothesis testing can be performed to evaluate whether specific predictors have a statistically significant effect on the response, and confidence intervals can also be constructed to quantify the uncertainty around the estimated parameters.\nTo ensure the validity of the model, certain technical conditions must be verified. These include linearity (the relationship between predictors and the response is linear), independence of errors, homoscedasticity (constant variance of the errors), and normality of the error terms. Diagnostic tools such as residual plots, normality tests, and quantile-quantile (Q-Q) plots are commonly used to assess these assumptions.\nIf the assumptions are not met, the model can be improved by proposing a more sophisticated structure (e.g., variable transformations, variance structures that do not assume independence or homoscedasticity, interactions between predictors, etc.). The model can also be estimated using techniques such as LASSO or Ridge, which generally provide estimators with lower variance.\nIn this illustrative case, only a basic linear regression model is fitted, which still achieves excellent performance."
  },
  {
    "objectID": "code/03_linear_regression.html#libraries-and-data",
    "href": "code/03_linear_regression.html#libraries-and-data",
    "title": "Salary prediction using a linear regression model",
    "section": "Libraries and data",
    "text": "Libraries and data\n\nimport sys\nimport os\nsys.path.append(os.path.join(os.getcwd(), \"code\"))\nfrom modulos import metrics, analyze_residuals, scatter_plot_real_vs_pred\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.stats.anova import anova_lm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import root_mean_squared_error, r2_score\nimport joblib\n\n# Cargar las particiones\ntrain_df = pd.read_csv('../data/train_set.csv')\ntest_df = pd.read_csv('../data/test_set.csv')"
  },
  {
    "objectID": "code/03_linear_regression.html#training",
    "href": "code/03_linear_regression.html#training",
    "title": "Salary prediction using a linear regression model",
    "section": "Training",
    "text": "Training\nThe model shows a very good performance, with \\(R^2: 0.923\\), which indicates that 92.3% of the variance in the response (salary) is explained by the predictors in the model.\nOn average, the salary predictions made by the model differ from the actual values by USD 13,372.68.\nThe model underestimates the response for outlier cases with very high real salaries.\n\n# Model fitting\nmodelo =  smf.ols('salary ~ age + gender + educ + title_cat + exp', data = train_df).fit()\nmodelo.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nsalary\nR-squared:\n0.923\n\n\nModel:\nOLS\nAdj. R-squared:\n0.921\n\n\nMethod:\nLeast Squares\nF-statistic:\n428.9\n\n\nDate:\nWed, 29 Jan 2025\nProb (F-statistic):\n2.12e-154\n\n\nTime:\n02:33:16\nLog-Likelihood:\n-3221.4\n\n\nNo. Observations:\n295\nAIC:\n6461.\n\n\nDf Residuals:\n286\nBIC:\n6494.\n\n\nDf Model:\n8\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-4.022e+04\n1.54e+04\n-2.616\n0.009\n-7.05e+04\n-9962.527\n\n\ngender[T.Male]\n7998.6856\n1607.752\n4.975\n0.000\n4834.158\n1.12e+04\n\n\neduc[T.Master's]\n1.839e+04\n2115.086\n8.695\n0.000\n1.42e+04\n2.26e+04\n\n\neduc[T.PhD]\n2.318e+04\n2873.911\n8.066\n0.000\n1.75e+04\n2.88e+04\n\n\ntitle_cat[T.Leadership]\n1.405e+04\n3501.546\n4.012\n0.000\n7155.489\n2.09e+04\n\n\ntitle_cat[T.Other]\n375.2237\n2746.080\n0.137\n0.891\n-5029.868\n5780.315\n\n\ntitle_cat[T.Senior]\n1.359e+04\n2640.661\n5.146\n0.000\n8390.865\n1.88e+04\n\n\nage\n2540.9370\n565.687\n4.492\n0.000\n1427.499\n3654.375\n\n\nexp\n2497.2653\n641.125\n3.895\n0.000\n1235.344\n3759.187\n\n\n\n\n\n\n\n\nOmnibus:\n56.291\nDurbin-Watson:\n1.626\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n289.301\n\n\nSkew:\n0.642\nProb(JB):\n1.51e-63\n\n\nKurtosis:\n7.678\nCond. No.\n774.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# Save predictions in train data\ntrain_df['pred_salary'] = modelo.predict(train_df)\n\n# Performance metrics\n# puede haber NaN en casos con datos faltantes en predictoras\ntrain_sin_na_pred = train_df.dropna(subset = 'pred_salary') \nmetrics(train_sin_na_pred['salary'], train_sin_na_pred['pred_salary'], \"train\")\n\nMétricas para train:\n - R2: 0.9231\n - RMSE: 13372.6827\n\n\n\n\nscatter_plot_real_vs_pred(train_df, 'salary', 'pred_salary', \"Training data\", label = \" - Salary (USD)\")"
  },
  {
    "objectID": "code/03_linear_regression.html#inference",
    "href": "code/03_linear_regression.html#inference",
    "title": "Salary prediction using a linear regression model",
    "section": "Inference",
    "text": "Inference\nThe overall model is highly significant, as indicated by the F-test (F-stat: 428.9, p-value &lt; 0.0001), which means the predictors collectively explain the dependent variable significantly better than a model without them.\nAdditionally, each predictor individually makes a significant contribution to the model, considering the presence of the other variables, as indicated by the F-tests summarized in the ANOVA table.\nFrom the estimation of the coefficients of the model it can be concluded with a 95% of confidence that, holding other factors constant:\n\nEach additional year of age increases salary by an amount between USD 1,427 and USD 3,654.\nEach additional year of experience increases salary by an amount between USD 1,235 and USD 3,759.\nMen earn between USD 4,834 and USD 11,200 more than women.\nCompared to having a Bachelor’s degree, a Master’s degree increases salary by an amount between USD 14,200 and USD 22,600, and a PhD increases it by an amount between USD 17,500 and USD 28,800.\nCompared to job positions related to being “Junior”, the increase in salaries lies between USD 8,390 and USD 18,800 for “Senior”, and between USD 7155 and USD 20,900 for “Leadership”. Other job positions don’t differ significantly from “Junior”.\n\n\nprint(f\"Test Global F: {modelo.summary().tables[0].data[0][3]}\")\nanova_lm(modelo)\n\nTest Global F:    0.923\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\ngender\n1.0\n3.739986e+09\n3.739986e+09\n20.275749\n9.773076e-06\n\n\neduc\n2.0\n3.227025e+11\n1.613512e+11\n874.740489\n1.321158e-122\n\n\ntitle_cat\n3.0\n1.718697e+11\n5.728990e+10\n310.588252\n1.254207e-89\n\n\nage\n1.0\n1.317438e+11\n1.317438e+11\n714.228383\n9.819006e-80\n\n\nexp\n1.0\n2.798577e+09\n2.798577e+09\n15.172047\n1.222542e-04\n\n\nResidual\n286.0\n5.275445e+10\n1.844561e+08\nNaN\nNaN"
  },
  {
    "objectID": "code/03_linear_regression.html#evaluation-of-technical-conditions",
    "href": "code/03_linear_regression.html#evaluation-of-technical-conditions",
    "title": "Salary prediction using a linear regression model",
    "section": "Evaluation of technical conditions",
    "text": "Evaluation of technical conditions\nAn exploratory residual analysis was conducted, and no significant violations of the model assumptions were detected, allowing the inferential conclusions to be considered valid. The residual distribution appears to have slightly heavy tails, but this is minor.\n\nanalyze_residuals(modelo)"
  },
  {
    "objectID": "code/03_linear_regression.html#prediction-in-test-data",
    "href": "code/03_linear_regression.html#prediction-in-test-data",
    "title": "Salary prediction using a linear regression model",
    "section": "Prediction in test data",
    "text": "Prediction in test data\nThe model shows a very good performance when predicting salaries with new data, with \\(R^2: 0.8912\\), which indicates that 89.12% of the variance in the response (salary) is explained by the predictors in the model.\nOn average, the salary predictions made by the model with new data differ from the actual values by USD 15,819.82.\nAs expected, these metrics are slightly worse than the ones for the train data, with no indication of overfitting.\n\n# Métricas para el conjunto de prueba\ntest_df['pred_salary'] = modelo.predict(test_df)\ntest_sin_na_pred = test_df.dropna(subset = 'pred_salary')\nmetrics(test_sin_na_pred['salary'], test_sin_na_pred['pred_salary'], \"test\")\n\nMétricas para test:\n - R2: 0.8912\n - RMSE: 15819.8234\n\n\n\n\nscatter_plot_real_vs_pred(test_df, 'salary', 'pred_salary', \"Testing data\", label = \" - Salary (USD)\")"
  },
  {
    "objectID": "code/03_linear_regression.html#saving-the-trained-model",
    "href": "code/03_linear_regression.html#saving-the-trained-model",
    "title": "Salary prediction using a linear regression model",
    "section": "Saving the trained model",
    "text": "Saving the trained model\n\njoblib.dump(modelo, '../model_outputs/mod_reg_lin.pkl')\n\n['../model_outputs/mod_reg_lin.pkl']"
  },
  {
    "objectID": "code/01_exploratory_analysis.html",
    "href": "code/01_exploratory_analysis.html",
    "title": "Exploratory analysis",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to conduct a brief exploratory analysis of the cleaned data to describe the distributions of the available variables, as well as the nature of the possible association between salary and each of the others, using figures and tables with numerical summaries.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\nThe dataset reveals that individuals are on average 37.4 years old with 10 years of experience, earning approximately USD 100k, with salary positively associated with age and experience, higher for males, those with higher education, and those in “Senior” or “Leadership” roles.\n\n\n\n\nCode\nimport sys\nimport os\nsys.path.append(os.path.join(os.getcwd(), \"code\"))\nfrom modulos import create_frequency_table\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.tools as tls\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\n\n# Cargar los datos\ndatos = pd.read_csv('../data/clean_data.csv')"
  },
  {
    "objectID": "code/01_exploratory_analysis.html#summary",
    "href": "code/01_exploratory_analysis.html#summary",
    "title": "Exploratory analysis",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of this script is to conduct a brief exploratory analysis of the cleaned data to describe the distributions of the available variables, as well as the nature of the possible association between salary and each of the others, using figures and tables with numerical summaries.\n\n\n\n\n\n\n\n\nRemarks\n\n\n\nThe dataset reveals that individuals are on average 37.4 years old with 10 years of experience, earning approximately USD 100k, with salary positively associated with age and experience, higher for males, those with higher education, and those in “Senior” or “Leadership” roles.\n\n\n\n\nCode\nimport sys\nimport os\nsys.path.append(os.path.join(os.getcwd(), \"code\"))\nfrom modulos import create_frequency_table\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.tools as tls\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, HTML\n\n# Cargar los datos\ndatos = pd.read_csv('../data/clean_data.csv')"
  },
  {
    "objectID": "code/01_exploratory_analysis.html#exploration-of-each-variable-individually",
    "href": "code/01_exploratory_analysis.html#exploration-of-each-variable-individually",
    "title": "Exploratory analysis",
    "section": "Exploration of each variable individually",
    "text": "Exploration of each variable individually\nThe individuals in the dataset are, on average, 37.4 years old, with 10.0 years of work experience and a salary of approximately USD 100k. The gender is evenly distributed. More than half have only achieved a Bachelor’s degree level. About 60% hold a job with some level of hierarchy, as their job titles are related to the words “Senior” or “Leadership”.\n\n\nCode\n# Crear una figura con 1 fila y 3 columnas\nplt.figure(figsize=(7, 2.5))\n\n# Primer gráfico de densidad para 'age'\nplt.subplot(1, 3, 1)  \nsns.kdeplot(datos['age'], fill=True, color=\"blue\", cut = 0)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")\n# plt.grid(True)\nplt.gca().get_yaxis().set_visible(False)\nplt.xticks(ticks=(range(20, 60, 10)))\n\n# Segundo gráfico de densidad para 'exp'\nplt.subplot(1, 3, 2) \nsns.kdeplot(datos['exp'], fill=True, color=\"blue\", cut = 0)\nplt.xlabel(\"Years of experience\")\nplt.ylabel(\"Density\")\n# plt.grid(True)\nplt.gca().get_yaxis().set_visible(False)\nplt.xticks(ticks=(range(0, 30, 5)))\n\n# Tercer gráfico de densidad para 'salary'\nplt.subplot(1, 3, 3)  \nsns.kdeplot(datos['salary'], fill=True, color=\"blue\", cut = 0)\nplt.xlabel(\"Salary (USD)\")\nplt.ylabel(\"Density\")\n# plt.grid(True)\nplt.gca().get_yaxis().set_visible(False)\nplt.xticks(ticks=(range(25000, 176000, 75000)))\n\nplt.suptitle(\"Distribution of age, years of experience and salary\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Figura de barras de variables categóricas\nfig_bar = make_subplots(rows=1, cols=3)\n\n# Calcular proporciones y agregar trazas\n\n# Diagrama de barras para 'gender'\ngender_counts = datos['gender'].value_counts(normalize=True) * 100\nfig_bar.add_trace(\n    go.Bar(x=gender_counts.index, y=gender_counts.values, marker_color='blue', name='Gender'),\n    row=1, col=1\n)\n\n# Diagrama de barras para 'educ'\neduc_counts = datos['educ'].value_counts(normalize=True) * 100\nfig_bar.add_trace(\n    go.Bar(x=educ_counts.index, y=educ_counts.values, marker_color='blue', name='Education'),\n    row=1, col=2\n)\n\n# Diagrama de barras para 'title_cat'\ntitle_cat_counts = datos['title_cat'].value_counts(normalize=True) * 100\nfig_bar.add_trace(\n    go.Bar(x=title_cat_counts.index, y=title_cat_counts.values, marker_color='blue', name='Title Category'),\n    row=1, col=3\n)\n\n# Actualizar el diseño\nfig_bar.update_layout(\n    title_text=\"Distribution of gender, education and word related to job title\",\n    showlegend=False,\n    height=250,\n    width=750,\n    title_font=dict(size=18),\n    yaxis1_title_text=\"Percentage\",\n    xaxis1_title_text = \"Gender\",\n    xaxis2_title_text = \"Education\",\n    xaxis3_title_text = \"Job title related to\",\n    yaxis1_ticksuffix = \"%\",\n    yaxis2_ticksuffix = \"%\",\n    yaxis3_ticksuffix = \"%\"\n)\n\n# Mostrar figura\nfig_bar.show()\n\n\n                                                \n\n\nStatistics for quantitative variables\n\n\nCode\nnumeric_stats = datos[['age', 'exp', 'salary']].describe().transpose()\nnumeric_stats['IQR'] = numeric_stats['75%'] - numeric_stats['25%']\nnumeric_stats = numeric_stats[['min', 'max', 'mean', '50%', 'std', 'IQR']]\nnumeric_stats.columns = ['Min', 'Max', 'Mean', 'Median', 'St. Dev.', 'IQR']\n\n# Mostrar el cuadro con estadísticas\nnumeric_stats.style.format(precision=2)\n\n\n\n\n\n\n\n \nMin\nMax\nMean\nMedian\nSt. Dev.\nIQR\n\n\n\n\nage\n23.00\n53.00\n37.41\n36.00\n7.07\n13.00\n\n\nexp\n0.00\n25.00\n10.03\n9.00\n6.56\n11.00\n\n\nsalary\n30000.00\n250000.00\n100670.24\n95000.00\n48079.58\n85000.00\n\n\n\n\n\nFrequency tables for categorical variables\n\n\nCode\n# Crear tablas para cada variable categórica\ntabla_gender = create_frequency_table(datos[\"gender\"], \"Gender\")\ntabla_educ = create_frequency_table(datos[\"educ\"], \"Education\")\ntabla_title_cat = create_frequency_table(datos[\"title_cat\"], \"Job Title related to\")\n\n\n\n\n\n\nCode\nHTML(tabla_gender.to_html(index=False))\n\n\n\n\n\nGender\nN\nPercentage\n\n\n\n\nMale\n193\n51.74\n\n\nFemale\n177\n47.45\n\n\nNaN\n3\n0.80\n\n\nTotal\n373\n99.99\n\n\n\n\n\n\n\n\n\n\nCode\nHTML(tabla_educ.to_html(index=False))\n\n\n\n\n\nEducation\nN\nPercentage\n\n\n\n\nBachelor's\n224\n60.05\n\n\nMaster's\n98\n26.27\n\n\nPhD\n51\n13.67\n\n\nTotal\n373\n99.99\n\n\n\n\n\n\n\n\n\n\nCode\nHTML(tabla_title_cat.to_html(index=False))\n\n\n\n\n\nJob Title related to\nN\nPercentage\n\n\n\n\nSenior\n152\n40.75\n\n\nJunior\n90\n24.13\n\n\nLeadership\n77\n20.64\n\n\nOther\n54\n14.48\n\n\nTotal\n373\n100.00"
  },
  {
    "objectID": "code/01_exploratory_analysis.html#relationship-between-salary-and-the-other-variables",
    "href": "code/01_exploratory_analysis.html#relationship-between-salary-and-the-other-variables",
    "title": "Exploratory analysis",
    "section": "Relationship between salary and the other variables",
    "text": "Relationship between salary and the other variables\nThe salary is strongly and positively associated with both the person’s age and years of experience. The salary distribution appears to be right-skewed for males compared to females. A clear salary increase is observed for higher education levels, as well as for positions linked to “Leadership” or “Senior”.\n\n\nCode\n# Crear la figura con dos paneles (subplots)\nfig = make_subplots(rows=1, cols=2)\n\n# Agregar el gráfico de dispersión para 'salary' vs 'age'\nfig.add_trace(\n    go.Scatter(\n        x=datos['age'],\n        y=datos['salary'],\n        mode='markers',  # Puntos\n        marker=dict(color='blue', opacity=0.5),  # Color azul y transparencia\n    ),\n    row=1, col=1\n)\n\n# Agregar el gráfico de dispersión para 'salary' vs 'exp'\nfig.add_trace(\n    go.Scatter(\n        x=datos['exp'],\n        y=datos['salary'],\n        mode='markers',  # Puntos\n        marker=dict(color='blue', opacity=0.5),  # Color azul y transparencia\n    ),\n    row=1, col=2\n)\n\n# Actualizar los ejes y la configuración del gráfico\nfig.update_layout(\n    showlegend=False,  \n    xaxis_title=\"Age\", \n    yaxis_title=\"Salary\", \n    yaxis2_title=\"Salary\", \n    xaxis2_title=\"Years of Experience\", \n    # template=\"plotly_white\",\n    title_text=\"Salary vs age and years of experience\",\n    width = 600,\n    height = 250\n)\n\n# Mostrar la figura\nfig.show()\n\n\n                                                \n\n\n\n\nCode\n# Figura con boxplots de salary vs categorical variables\nfig_box = make_subplots(rows=3, cols=1)\n\n# Boxplot de 'salary' y 'gender'\nfig_box.add_trace(\n    go.Box(y=datos['gender'], x=datos['salary'], name='Gender', orientation = 'h', marker=dict(color='blue')),\n    row=1, col=1\n)\n\n# Boxplot de 'salary' y 'educ'\nfig_box.add_trace(\n    go.Box(y=datos['educ'], x=datos['salary'], name='Education',  orientation = 'h', marker=dict(color='blue')),\n    row=2, col=1\n)\n\n# Boxplot de 'salary' y 'title_cat'\nfig_box.add_trace(\n    go.Box(y=datos['title_cat'], x=datos['salary'], name='Title Category', orientation = 'h', marker=dict(color='blue')),\n    row=3, col=1\n)\n\nfig_box.update_layout(\n    title_text=\"Salary by gender, education and job title\",\n    xaxis3_title=\"Salary\",\n    yaxis1_title = \"Gender\",\n    yaxis2_title = \"Education\",\n    yaxis3_title = \"Job title related to\",\n    showlegend=False,\n    width = 450,\n    height = 450\n)\nfig_box.show()"
  }
]